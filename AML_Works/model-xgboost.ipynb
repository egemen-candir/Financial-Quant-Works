{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"AML DETECTION WITH XGBOOST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv('HI-Small_Trans_with_sanctions.csv')\n",
    "print(f\"Data loaded: {df.shape[0]:,} transactions\")\n",
    "print(f\"Target distribution:\\n{df['Is Laundering'].value_counts()}\")\n",
    "\n",
    "# 2. Define sanctions features FIRST THING\n",
    "sanctions_features = ['from_bank_sanctioned', 'to_bank_sanctioned', 'sanctions_exposure_flag']\n",
    "available_sanctions_features = [col for col in sanctions_features if col in df.columns]\n",
    "print(f\"Available sanctions features: {available_sanctions_features}\")\n",
    "\n",
    "# Check sanctions features\n",
    "if 'sanctions_exposure_flag' in df.columns:\n",
    "    sanctions_count = df['sanctions_exposure_flag'].sum()\n",
    "    print(f\"Sanctions exposure: {sanctions_count:,} transactions ({sanctions_count/len(df)*100:.3f}%)\")\n",
    "    if 'from_bank_sanctioned' in df.columns and 'to_bank_sanctioned' in df.columns:\n",
    "        from_sanctioned = df['from_bank_sanctioned'].sum()\n",
    "        to_sanctioned = df['to_bank_sanctioned'].sum()\n",
    "        print(f\"  ‚Ä¢ From sanctioned banks: {from_sanctioned:,}\")\n",
    "        print(f\"  ‚Ä¢ To sanctioned banks: {to_sanctioned:,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Sanctions features not found in dataset\")\n",
    "\n",
    "# 3. Feature Engineering\n",
    "print(\"\\nCreating AML-specific features...\")\n",
    "\n",
    "# Convert timestamp\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df['hour'] = df['Timestamp'].dt.hour\n",
    "df['day_of_week'] = df['Timestamp'].dt.dayofweek\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Amount-based features\n",
    "df['amount_difference'] = abs(df['Amount Received'] - df['Amount Paid'])\n",
    "df['amount_ratio'] = df['Amount Received'] / (df['Amount Paid'] + 1e-8)\n",
    "df['log_amount_received'] = np.log1p(df['Amount Received'])\n",
    "df['log_amount_paid'] = np.log1p(df['Amount Paid'])\n",
    "\n",
    "# Currency features\n",
    "df['currency_mismatch'] = (df['Receiving Currency'] != df['Payment Currency']).astype(int)\n",
    "\n",
    "# Bank features\n",
    "df['same_bank'] = (df['From Bank'] == df['To Bank']).astype(int)\n",
    "\n",
    "# Account features\n",
    "le_account = LabelEncoder()\n",
    "le_account1 = LabelEncoder()\n",
    "le_payment_format = LabelEncoder()\n",
    "le_recv_currency = LabelEncoder()\n",
    "le_pay_currency = LabelEncoder()\n",
    "\n",
    "df['account_encoded'] = le_account.fit_transform(df['Account'].astype(str))\n",
    "df['account1_encoded'] = le_account1.fit_transform(df['Account.1'].astype(str))\n",
    "df['payment_format_encoded'] = le_payment_format.fit_transform(df['Payment Format'])\n",
    "df['recv_currency_encoded'] = le_recv_currency.fit_transform(df['Receiving Currency'])\n",
    "df['pay_currency_encoded'] = le_pay_currency.fit_transform(df['Payment Currency'])\n",
    "\n",
    "# Risk indicators\n",
    "df['round_amount_received'] = (df['Amount Received'] % 1000 == 0).astype(int)\n",
    "df['round_amount_paid'] = (df['Amount Paid'] % 1000 == 0).astype(int)\n",
    "df['high_risk_hours'] = ((df['hour'] < 6) | (df['hour'] > 22)).astype(int)\n",
    "\n",
    "# Additional XGBoost-friendly features\n",
    "df['bank_pair'] = df['From Bank'].astype(str) + '_' + df['To Bank'].astype(str)\n",
    "df['bank_pair_encoded'] = LabelEncoder().fit_transform(df['bank_pair'])\n",
    "\n",
    "# 4. Select features\n",
    "feature_columns = [\n",
    "    'From Bank', 'To Bank', 'bank_pair_encoded',\n",
    "    'Amount Received', 'Amount Paid', \n",
    "    'log_amount_received', 'log_amount_paid',\n",
    "    'amount_difference', 'amount_ratio',\n",
    "    'currency_mismatch', 'same_bank',\n",
    "    'hour', 'day_of_week', 'is_weekend', 'high_risk_hours',\n",
    "    'round_amount_received', 'round_amount_paid',\n",
    "    'payment_format_encoded', 'recv_currency_encoded', 'pay_currency_encoded',\n",
    "    'account_encoded', 'account1_encoded'\n",
    "]\n",
    "\n",
    "# Add sanctions features\n",
    "feature_columns.extend(available_sanctions_features)\n",
    "\n",
    "if available_sanctions_features:\n",
    "    print(f\"Added sanctions features: {available_sanctions_features}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No sanctions features found in dataset\")\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "y = df['Is Laundering'].copy()\n",
    "\n",
    "print(f\"Features selected: {len(feature_columns)}\")\n",
    "print(f\"Class balance - Legitimate: {(y==0).sum():,}, Laundering: {(y==1).sum():,}\")\n",
    "\n",
    "# 5. Handle missing values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# 6. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]:,} transactions\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} transactions\")\n",
    "\n",
    "# 7. Calculate scale_pos_weight\n",
    "num_negative = (y_train == 0).sum()\n",
    "num_positive = (y_train == 1).sum()\n",
    "scale_pos_weight = num_negative / num_positive\n",
    "\n",
    "print(f\"\\nClass imbalance handling:\")\n",
    "print(f\"  ‚Ä¢ Negative samples: {num_negative:,}\")\n",
    "print(f\"  ‚Ä¢ Positive samples: {num_positive:,}\")\n",
    "print(f\"  ‚Ä¢ scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# 8. Train XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 9. Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 10. Evaluate performance\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"XGBOOST MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nROC AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives: {cm[0,0]:,}\")\n",
    "print(f\"False Positives: {cm[0,1]:,}\")\n",
    "print(f\"False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"True Positives: {cm[1,1]:,}\")\n",
    "\n",
    "# 11. Feature Importance\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE (TOP 15)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(15).iterrows()):\n",
    "    print(f\"{i+1:2d}. {row['feature']:<25} (importance: {row['importance']:.4f})\")\n",
    "\n",
    "# Sanctions feature analysis\n",
    "if available_sanctions_features:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SANCTIONS FEATURE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for feature in available_sanctions_features:\n",
    "        if feature in feature_importance['feature'].values:\n",
    "            importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]\n",
    "            rank = feature_importance[feature_importance['feature'] == feature].index[0] + 1\n",
    "            \n",
    "            # Calculate correlation with laundering\n",
    "            sanctions_laundering_rate = df[df[feature] == 1]['Is Laundering'].mean()\n",
    "            normal_laundering_rate = df[df[feature] == 0]['Is Laundering'].mean()\n",
    "            risk_multiplier = sanctions_laundering_rate / normal_laundering_rate if normal_laundering_rate > 0 else float('inf')\n",
    "            \n",
    "            print(f\"\\nüö® {feature.upper()}:\")\n",
    "            print(f\"  ‚Ä¢ Feature importance rank: #{rank} (score: {importance:.4f})\")\n",
    "            print(f\"  ‚Ä¢ Laundering rate with sanctions exposure: {sanctions_laundering_rate:.4%}\")\n",
    "            print(f\"  ‚Ä¢ Laundering rate without sanctions exposure: {normal_laundering_rate:.4%}\")\n",
    "            print(f\"  ‚Ä¢ Risk multiplier: {risk_multiplier:.2f}x\")\n",
    "\n",
    "# 12. Threshold Analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "print(f\"\\nüö® ALERT VOLUME AT DIFFERENT RISK THRESHOLDS:\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    thresh_pred = (y_pred_proba > threshold).astype(int)\n",
    "    alerts = thresh_pred.sum()\n",
    "    alert_rate = alerts / len(y_test) * 100\n",
    "    \n",
    "    thresh_cm = confusion_matrix(y_test, thresh_pred)\n",
    "    thresh_precision = thresh_cm[1,1] / (thresh_cm[1,1] + thresh_cm[0,1]) if (thresh_cm[1,1] + thresh_cm[0,1]) > 0 else 0\n",
    "    thresh_recall = thresh_cm[1,1] / (thresh_cm[1,1] + thresh_cm[1,0]) if (thresh_cm[1,1] + thresh_cm[1,0]) > 0 else 0\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Threshold {threshold}: {alerts:,} alerts ({alert_rate:.2f}%) | Precision: {thresh_precision:.3f} | Recall: {thresh_recall:.3f}\")\n",
    "\n",
    "# 13. Visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(2, 3, 1)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, color='blue', linewidth=2, label=f'XGBoost (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - XGBoost AML Detection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature Importance\n",
    "plt.subplot(2, 3, 2)\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Prediction Distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.7, label='Legitimate', density=True)\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.7, label='Laundering', density=True)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Prediction Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# XGBoost built-in feature importance\n",
    "plt.subplot(2, 3, 4)\n",
    "xgb.plot_importance(xgb_model, max_num_features=10, importance_type='weight', ax=plt.gca())\n",
    "plt.title('XGBoost Feature Importance (Weight)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 14. Business Interpretation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUSINESS INTERPRETATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_transactions = len(y_test)\n",
    "flagged_transactions = (y_pred_proba > 0.5).sum()\n",
    "flagged_rate = flagged_transactions / total_transactions * 100\n",
    "\n",
    "print(f\"\\nüìä OPERATIONAL METRICS:\")\n",
    "print(f\"  ‚Ä¢ Total transactions analyzed: {total_transactions:,}\")\n",
    "print(f\"  ‚Ä¢ Transactions flagged as suspicious: {flagged_transactions:,} ({flagged_rate:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Model accuracy: {(y_pred == y_test).mean()*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "top_risk_factor = feature_importance.iloc[0]\n",
    "print(f\"  ‚Ä¢ Most predictive feature: {top_risk_factor['feature']} (importance: {top_risk_factor['importance']:.4f})\")\n",
    "print(f\"  ‚Ä¢ XGBoost automatically captures feature interactions\")\n",
    "print(f\"  ‚Ä¢ Model handles class imbalance with scale_pos_weight = {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Sanctions insights\n",
    "if available_sanctions_features:\n",
    "    sanctions_in_top10 = any(feat in feature_importance.head(10)['feature'].values for feat in available_sanctions_features)\n",
    "    if sanctions_in_top10:\n",
    "        print(f\"  ‚Ä¢ Sanctions features ranked in top 10 most important features\")\n",
    "    \n",
    "    if 'sanctions_exposure_flag' in df.columns:\n",
    "        sanctions_impact = df[df['sanctions_exposure_flag'] == 1]['Is Laundering'].mean()\n",
    "        baseline_rate = df[df['sanctions_exposure_flag'] == 0]['Is Laundering'].mean()\n",
    "        if baseline_rate > 0:\n",
    "            multiplier = sanctions_impact / baseline_rate\n",
    "            print(f\"  ‚Ä¢ Sanctions exposure increases laundering risk by {multiplier:.1f}x\")\n",
    "\n",
    "print(f\"\\nüéØ ADVANTAGES OVER LOGISTIC REGRESSION:\")\n",
    "print(f\"  ‚Ä¢ No feature scaling required\")\n",
    "print(f\"  ‚Ä¢ Automatic interaction detection\")\n",
    "print(f\"  ‚Ä¢ Better handling of non-linear patterns\")\n",
    "print(f\"  ‚Ä¢ Built-in feature importance ranking\")\n",
    "print(f\"  ‚Ä¢ Early stopping prevents overfitting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
